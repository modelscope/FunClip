import gradio as gr
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from videoclipper import VideoClipper


if __name__ == "__main__":
    inference_pipeline = pipeline(
        task=Tasks.auto_speech_recognition,
        model='damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch',
        vad_model='damo/speech_fsmn_vad_zh-cn-16k-common-pytorch',
        punc_model='damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch',
    )
    audio_clipper = VideoClipper(inference_pipeline)

    def audio_recog(audio_input):
        return audio_clipper.recog(audio_input)

    def audio_clip(dest_text, start_ost, end_ost, state):
        return audio_clipper.clip(dest_text, start_ost, end_ost, state)

    def video_recog(video_input):
        return audio_clipper.video_recog(video_input)

    def video_clip(dest_text, start_ost, end_ost, state):
        return audio_clipper.video_clip(dest_text, start_ost, end_ost, state)

    def video_clip_addsub(dest_text, start_ost, end_ost, state, font_size, font_color):
        return audio_clipper.video_clip(dest_text, start_ost, end_ost, state, font_size, font_color, add_sub=True)

    '''
    top_md_1 = ("""
    åŸºäºè¾¾æ‘©é™¢è‡ªç ”Paraformer-é•¿éŸ³é¢‘ç‰ˆçš„è¯­éŸ³è¯†åˆ«ã€ç«¯ç‚¹æ£€æµ‹ã€æ ‡ç‚¹é¢„æµ‹ã€æ—¶é—´æˆ³åŠŸèƒ½

    å‡†ç¡®è¯†åˆ«ï¼Œè‡ªç”±å¤åˆ¶æ‰€éœ€æ®µè½å¹¶ä¸€é”®è£å‰ªã€æ·»åŠ å­—å¹•

    * Step1: ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼ˆæˆ–ä½¿ç”¨ä¸‹æ–¹çš„ç”¨ä¾‹ä½“éªŒï¼‰ï¼Œç‚¹å‡» **<font color="#f7802b">è¯†åˆ«</font>** æŒ‰é’®
    * Step2: å¤åˆ¶è¯†åˆ«ç»“æœä¸­æ‰€éœ€çš„æ–‡å­—è‡³å³ä¸Šæ–¹ï¼Œè®¾ç½®åç§»ä¸å­—å¹•é…ç½®ï¼ˆå¯é€‰ï¼‰
    * Step3: ç‚¹å‡» **<font color="#f7802b">è£å‰ª</font>** æŒ‰é’®æˆ– **<font color="#f7802b">è£å‰ªå¹¶æ·»åŠ å­—å¹•</font>** æŒ‰é’®è·å¾—ç»“æœ
    """)
    '''

    top_md_2 = ("""
    å—åˆ°ç½‘ç»œä¼ è¾“ä¸æœåŠ¡èµ„æºçš„é™åˆ¶ï¼Œç”¨äºä½“éªŒçš„è§†é¢‘æœ€å¥½å¤§å°åœ¨40mbä»¥ä¸‹
    è¿‡å¤§çš„è§†é¢‘å¯ä»¥å°è¯•åˆ†ç¦»éŸ³è½¨ä½¿ç”¨éŸ³é¢‘å‰ªè¾‘ï¼Œæˆ– **<font color="#1785c4">é€šè¿‡æºä»£ç å°†æ‚¨çš„ClipVideoæœåŠ¡éƒ¨ç½²åœ¨æœ¬åœ°ï¼ˆæ¨èï¼‰</font>** ï¼š
    <div align="center">
    <div style="display:flex; gap: 0.25rem;" align="center">
    FunASR_APP: <a href='https://github.com/alibaba/funasr-app'><img src='https://img.shields.io/badge/Github-Code-blue'></a> 
    ğŸŒŸæ”¯æŒæˆ‘ä»¬: <a href='https://github.com/alibaba/funasr-app/stargazers'><img src='https://img.shields.io/github/stars/alibaba/funasr-app.svg?style=social'></a>
    </div>
    </div>
    """)

    top_md_3 = ("""è®¿é—®FunASRé¡¹ç›®ä¸è®ºæ–‡èƒ½å¤Ÿå¸®åŠ©æ‚¨æ·±å…¥äº†è§£ClipVideoä¸­æ‰€ä½¿ç”¨çš„è¯­éŸ³å¤„ç†ç›¸å…³æ¨¡å‹ï¼š
    <div align="center">
    <div style="display:flex; gap: 0.25rem;" align="center">
        FunASR: <a href='https://github.com/alibaba-damo-academy/FunASR'><img src='https://img.shields.io/badge/Github-Code-blue'></a> 
        FunASR Paper: <a href="https://arxiv.org/abs/2305.11013"><img src="https://img.shields.io/badge/Arxiv-2305.11013-orange"></a> 
        ğŸŒŸStar FunASR: <a href='https://github.com/alibaba-damo-academy/FunASR/stargazers'><img src='https://img.shields.io/github/stars/alibaba-damo-academy/FunASR.svg?style=social'></a>
    </div>
    </div>
    """)

    # gradio interface
    with gr.Blocks() as demo:
        #gr.Image("./examples/guide.png", show_label=False)
        # gr.Markdown(top_md_1)
        #gr.Markdown(top_md_2)
        #gr.Markdown(top_md_3)
        video_state = gr.State()
        audio_state = gr.State()
        with gr.Tab("ğŸ¥âœ‚ï¸è§†é¢‘è£å‰ª Video Clipping"):
            with gr.Row():
                with gr.Column():
                    video_input = gr.Video(label="ğŸ¥è§†é¢‘è¾“å…¥ Video Input")
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A4%9A%E8%AF%BB%E4%B9%A6%EF%BC%9F%E8%BF%99%E6%98%AF%E6%88%91%E5%90%AC%E8%BF%87%E6%9C%80%E5%A5%BD%E7%9A%84%E7%AD%94%E6%A1%88-%E7%89%87%E6%AE%B5.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/2022%E4%BA%91%E6%A0%96%E5%A4%A7%E4%BC%9A_%E7%89%87%E6%AE%B5.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/2022%E4%BA%91%E6%A0%96%E5%A4%A7%E4%BC%9A_%E7%89%87%E6%AE%B52.mp4', 
                                 'https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E4%BD%BF%E7%94%A8chatgpt_%E7%89%87%E6%AE%B5.mp4'],
                                [video_input])
                    recog_button2 = gr.Button("ğŸ‘‚è¯†åˆ« Recognize")
                    video_text_output = gr.Textbox(label="âœï¸è¯†åˆ«ç»“æœ Recognition Result")
                    video_srt_output = gr.Textbox(label="ğŸ“–SRTå­—å¹•å†…å®¹ RST Subtitles")
                with gr.Column():
                    video_text_input = gr.Textbox(label="âœï¸å¾…è£å‰ªæ–‡æœ¬ Text to Clip (å¤šæ®µæ–‡æœ¬ä½¿ç”¨'#'è¿æ¥)")
                    with gr.Row():
                        video_start_ost = gr.Slider(minimum=-500, maximum=1000, value=0, step=50, label="âªå¼€å§‹ä½ç½®åç§» Start Offset (ms)")
                        video_end_ost = gr.Slider(minimum=-500, maximum=1000, value=100, step=50, label="â©ç»“æŸä½ç½®åç§» End Offset (ms)")
                    with gr.Row():
                        font_size = gr.Slider(minimum=10, maximum=100, value=32, step=2, label="ğŸ” å­—å¹•å­—ä½“å¤§å° Subtitle Font Size")
                        font_color = gr.Radio(["black", "white", "green", "red"], label="ğŸŒˆå­—å¹•é¢œè‰² Subtitle Color", value='white')
                        # font = gr.Radio(["é»‘ä½“", "Alibaba Sans"], label="å­—ä½“ Font")
                    with gr.Row():
                        clip_button2 = gr.Button("âœ‚ï¸è£å‰ª\nClip")
                        clip_button3 = gr.Button("âœ‚ï¸è£å‰ªå¹¶æ·»åŠ å­—å¹•\nClip and Generate Subtitles")
                    video_output = gr.Video(label="ğŸ¥è£å‰ªç»“æœ Audio Clipped")
                    video_mess_output = gr.Textbox(label="â„¹ï¸è£å‰ªä¿¡æ¯ Clipping Log")
                    video_srt_clip_output = gr.Textbox(label="ğŸ“–è£å‰ªéƒ¨åˆ†SRTå­—å¹•å†…å®¹ Clipped RST Subtitles")

        with gr.Tab("ğŸ”Šâœ‚ï¸éŸ³é¢‘è£å‰ª Audio Clipping"):
            with gr.Row():
                with gr.Column():
                    audio_input = gr.Audio(label="ğŸ”ŠéŸ³é¢‘è¾“å…¥ Audio Input")
                    gr.Examples(['https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ClipVideo/%E9%B2%81%E8%82%83%E9%87%87%E8%AE%BF%E7%89%87%E6%AE%B51.wav'], [audio_input])
                    recog_button1 = gr.Button("ğŸ‘‚è¯†åˆ« Recognize")
                    audio_text_output = gr.Textbox(label="âœï¸è¯†åˆ«ç»“æœ Recognition Result")
                    audio_srt_output = gr.Textbox(label="ğŸ“–SRTå­—å¹•å†…å®¹ RST Subtitles")
                with gr.Column():
                    audio_text_input = gr.Textbox(label="âœï¸å¾…è£å‰ªæ–‡æœ¬ Text to Clip (å¤šæ®µæ–‡æœ¬ä½¿ç”¨'#'è¿æ¥)")
                    with gr.Row():
                        audio_start_ost = gr.Slider(minimum=-500, maximum=1000, value=0, step=50, label="âªå¼€å§‹ä½ç½®åç§» Start Offset (ms)")
                        audio_end_ost = gr.Slider(minimum=-500, maximum=1000, value=100, step=50, label="â©ç»“æŸä½ç½®åç§» End Offset (ms)")
                    with gr.Row():
                        clip_button1 = gr.Button("âœ‚ï¸è£å‰ª Clip")
                    audio_output = gr.Audio(label="ğŸ”Šè£å‰ªç»“æœ Audio Clipped")
                    audio_mess_output = gr.Textbox(label="â„¹ï¸è£å‰ªä¿¡æ¯ Clipping Log")
                    audio_srt_clip_output = gr.Textbox(label="ğŸ“–è£å‰ªéƒ¨åˆ†SRTå­—å¹•å†…å®¹ Clipped RST Subtitles")
        
        recog_button1.click(audio_recog, 
                            inputs=audio_input, 
                            outputs=[audio_text_output, audio_srt_output, audio_state])
        clip_button1.click(audio_clip, 
                           inputs=[audio_text_input, audio_start_ost, audio_end_ost, audio_state], 
                           outputs=[audio_output, audio_mess_output, audio_srt_clip_output])

        recog_button2.click(video_recog, 
                            inputs=video_input, 
                            outputs=[video_text_output, video_srt_output, video_state])
        clip_button2.click(video_clip, 
                           inputs=[video_text_input, video_start_ost, video_end_ost, video_state], 
                           outputs=[video_output, video_mess_output, video_srt_clip_output])
        clip_button3.click(video_clip_addsub, 
                           inputs=[video_text_input, video_start_ost, video_end_ost, video_state, font_size, font_color], 
                           outputs=[video_output, video_mess_output, video_srt_clip_output])
    
    # start gradio service in local
    demo.queue(concurrency_count=3).launch()
